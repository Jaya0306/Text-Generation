{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5b825f-f9ff-4318-8c36-6dcd9fb85f6a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e2b97-f572-4e28-936d-32901187b291",
   "metadata": {},
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3338384c-701e-4ad2-8079-ddf8c5ca49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844f17f4-7e6f-456c-8426-8c702a427ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ea329-9d7e-4148-8e20-da2d5afa5a32",
   "metadata": {},
   "source": [
    "### Download the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b5c524-a437-424c-8c1d-a08896e8419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce0c47-1ea0-404d-ae2b-dab5f87a4ef6",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11c110f-4532-402f-a29e-cded3645ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1fc2c-84bb-4e9e-822a-49d95a49c9dc",
   "metadata": {},
   "source": [
    "Let's take a look at the first 250 characters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af9b606-d83b-4392-ac6c-4be3d5d3d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d510e6-bc63-4436-8ced-51ed0f368775",
   "metadata": {},
   "source": [
    "Let's check to see how many unique characters are in our corpus/documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8751bb91-c546-4fcd-a130-84a61d5a3a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f\"{len(vocab)} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58121710-07ba-41bf-b05b-e6814317cdd2",
   "metadata": {},
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a578a-8efb-499b-bdeb-bff400aed088",
   "metadata": {},
   "source": [
    "### Vectorize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703d665-ffb1-47a0-8c1c-4d93d66d4a02",
   "metadata": {},
   "source": [
    "Before training, we need to convert the strings to a numerical representation.\n",
    "\n",
    "Using tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a10b309-6194-49a7-820c-fcd834b54d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = [\"abcdefg\", \"xyz\"]\n",
    "\n",
    "# TODO 1\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a973670-e094-4783-8e6a-fe6c78a313e1",
   "metadata": {},
   "source": [
    "Create the tf.keras.layers.StringLookup layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec0d870-4f66-4eb0-8f68-3bbe761d33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887ea1b-7d73-46f8-8f64-f90bdef5cd06",
   "metadata": {},
   "source": [
    "It converts from tokens to character IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a813a94-b78e-4ed1-98e0-fd915baaec2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401e95f-b753-43dd-8378-f7a89e61563a",
   "metadata": {},
   "source": [
    "Important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True).\n",
    "\n",
    "Note: Here instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e209eb3-a0d6-42ab-abf4-7630692e49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fc815-0c0b-434b-a75f-d05b970b2049",
   "metadata": {},
   "source": [
    "This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters:\n",
    "\n",
    "A RaggedTensor is a type of tensor in TensorFlow that can hold variable-length sequences — like sentences or words — without needing to pad them to the same length. This is crucial for text generation since each batch of text might produce outputs of varying lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c8f29c-377c-425b-8eed-f05b66af5eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2cac8-9e5e-4e8e-8ffc-13b401bd24a2",
   "metadata": {},
   "source": [
    "tf.strings.reduce_join to join the characters back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919573fe-02b3-495c-9c04-24fff1405eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255e87ef-5ab6-4939-8ddc-929c8eac3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a784b-effa-4cd0-9462-b51a5843c1a4",
   "metadata": {},
   "source": [
    "## The prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29173d-736a-4a36-98af-21b9b469ea41",
   "metadata": {},
   "source": [
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task we are training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f50a4c-e12a-47ca-b7c0-c92132643a8c",
   "metadata": {},
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "First use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f7c291-38f3-47c2-8996-7dd24be5e10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 2\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59bea8-87ab-4a83-8f7f-570c843da3cb",
   "metadata": {},
   "source": [
    "Creates a `tf.data.Dataset` where each item is a single ID (one per character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f70884-5464-4dc5-bee6-ba8d77f4765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dba6d87-807a-4e4f-aa85-a769b95047c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df16462-381a-482d-b198-8bda9f987f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11043"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba596ffa-ae71-4ca9-ada9-64342b5b6311",
   "metadata": {},
   "source": [
    "The `batch` method easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "519b1a9c-805e-43b5-bc7a-b8c3c409ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f006d-5eda-43c9-ae7a-3d8b0aa4948c",
   "metadata": {},
   "source": [
    "It's easier to see what this is doing if you join the tokens back into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b94d3b0d-70ac-4ef8-9a54-13112930af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bb63d-917a-4760-9d8b-11305bb96c6f",
   "metadata": {},
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
    "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
    "\n",
    "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe9d04f-971f-4c57-8d90-94b5766d0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1110bde8-46d7-4172-a2dc-7c5dae6329d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96191cb5-7f64-4e32-969b-76435e8da2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51573563-0b1f-4bcc-bc68-ffbfe630ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c495eb-ab14-41c2-abc2-18ee56f493bc",
   "metadata": {},
   "source": [
    "### Create training batches\n",
    "\n",
    "Used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e47ed9b-3ebd-4881-86ee-253e0831b2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573d700-cf0f-4957-b796-42e46b752a01",
   "metadata": {},
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62697f6a-0e43-432b-85ee-31c09c908050",
   "metadata": {},
   "source": [
    "A model with the following layers\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units`.\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ae7def-93d2-425b-9744-bc48a1b12aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42a224-3e98-4b2f-9a04-7c2e6579d731",
   "metadata": {},
   "source": [
    "The class below does the following:\n",
    "- We derive a class from tf.keras.Model\n",
    "- The constructor is used to define the layers of the model\n",
    "- We define the pass forward using the layers defined in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa6efd5-643c-4ec5-b464-6b4d6ee1b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__()\n",
    "        # TODO - Create an embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # TODO - Create a GRU layer\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units, return_sequences=True, return_state=True\n",
    "        )\n",
    "        # TODO - Finally connect it with a dense layer\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        # since we are training a text generation model,\n",
    "        # we use the previous state, in training. If there is no state,\n",
    "        # then we initialize the state\n",
    "        if states is None:\n",
    "            states = [tf.zeros((tf.shape(inputs)[0], self.gru.units))]\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec53f0-2b32-4651-aa9f-975dbfc76234",
   "metadata": {},
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b4836-b9ca-4f23-a700-ee5f33596eff",
   "metadata": {},
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b2f4bf2-bb29-4665-aa4f-d65d3a87cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7350bb9d-46da-4dfc-a4ad-91b63e12ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\n",
    "        example_batch_predictions.shape,\n",
    "        \"# (batch_size, sequence_length, vocab_size)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8815c47a-4043-4f99-9745-5a355821faf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf182ff-7487-477e-8dfb-0d0044f9f32a",
   "metadata": {},
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Trying it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73d61c8d-9356-402f-bde0-5f2dc512188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(\n",
    "    example_batch_predictions[0], num_samples=1\n",
    ")\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd789-a965-4557-9a9b-88a40eee666c",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6d79d39-3d6b-4e0e-a02e-23ea87d8e46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 64, 38, 54, 10, 20, 40, 10, 15, 48, 52, 56, 14, 65, 60, 34, 28,\n",
       "       45, 15,  9, 61, 31, 58, 15, 14, 45, 53, 34, 63, 48, 26, 58, 24, 36,\n",
       "        1,  4, 62, 53,  9, 27, 44,  3, 18, 52,  1, 19, 53, 54, 45, 33,  1,\n",
       "        9, 32, 37, 47, 51, 53, 51, 57,  3, 43,  3, 52, 11, 58, 40, 45, 63,\n",
       "       58, 51, 14, 43, 37, 50, 10, 45, 27, 15, 50, 43,  0, 50, 42,  5, 56,\n",
       "       57, 11, 54, 27,  7, 40, 36, 15, 63, 60, 23, 51, 23,  1, 36])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f2244-d28f-48ab-b840-b4c9278c2f40",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc3b1d0-0834-4709-9c94-168cbf074b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' Cominius\\nWith thee awhile: determine on some course,\\nMore than a wild exposture to each chance\\nThat'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'OyYo3Ga3BimqAzuUOfB.vRsBAfnUxiMsKW\\n$wn.Ne!Em\\nFnofT\\n.SXhlnlr!d!m:safxslAdXk3fNBkd[UNK]kc&qr:oN,aWBxuJlJ\\nW'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f36d83-ae07-4713-a906-f38c239556d2",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39394b18-8e5e-42cf-8082-84f10dc1b426",
   "metadata": {},
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66cc52-bda2-45ba-8aed-daf9cc669418",
   "metadata": {},
   "source": [
    "#### Attach an optimizer, and a loss function\n",
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because our model returns logits, we need to set the `from_logits` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "260c010d-553f-44b0-9db2-12fedd1d1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a loss function here\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24441dfb-5d09-453e-a795-ca16ea6ad78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190694, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\n",
    "    \"Prediction shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \" # (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617f411-d92c-488a-8d54-be57549a76db",
   "metadata": {},
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this we can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a6fbbe2-276f-4fcf-9c0c-611808db3684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(66.06862)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cebbfd-da60-4636-9911-5f8c4260bb6b",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. Using `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abc2702c-825f-40ae-a1ff-6c4317f512a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91f590-17c7-4e14-b4bb-b2eddb979212",
   "metadata": {},
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed6c71-ad7a-4ea4-addf-b4cc1882b416",
   "metadata": {},
   "source": [
    "Using a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15f3ee18-0482-4d1e-b2b2-452d6b29c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8df7f-a580-490b-b4e9-9a09d517b6f6",
   "metadata": {},
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4aea39-f205-45a3-88f9-7048e6fbc5e0",
   "metadata": {},
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea2e1fa-4d35-45aa-9be4-0d7798ba3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91345945-be68-48bf-b18f-e7bab1f956e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 3s/step - loss: 3.0430\n",
      "Epoch 2/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 3s/step - loss: 1.9070\n",
      "Epoch 3/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 3s/step - loss: 1.6233\n",
      "Epoch 4/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 3s/step - loss: 1.4756\n",
      "Epoch 5/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - loss: 1.3880\n",
      "Epoch 6/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 3s/step - loss: 1.3282\n",
      "Epoch 7/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 2s/step - loss: 1.2764\n",
      "Epoch 8/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 2s/step - loss: 1.2315\n",
      "Epoch 9/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 3s/step - loss: 1.1880\n",
      "Epoch 10/10\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 3s/step - loss: 1.1477\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acee631-d6ad-46a9-a1c7-74f376f73cd7",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7d741-e7a6-41b6-be85-5446390cce16",
   "metadata": {},
   "source": [
    "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
    "\n",
    "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941b298-de4a-4ade-8e91-8afbf8d80b6f",
   "metadata": {},
   "source": [
    "The following makes a single step prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ec58f69-eff6-4984-b057-8abb0fc6824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable(package=\"Custom\")\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # If components are not None, create the mask\n",
    "        if self.ids_from_chars is not None:\n",
    "            skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "            sparse_mask = tf.SparseTensor(\n",
    "                values=[-float(\"inf\")] * len(skip_ids),\n",
    "                indices=skip_ids,\n",
    "                dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
    "            )\n",
    "            self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "        else:\n",
    "            self.prediction_mask = None\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "\n",
    "        if self.prediction_mask is not None:\n",
    "            predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        return predicted_chars, states\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Initialize with placeholders; real components reattached after loading\n",
    "        return cls(model=None, chars_from_ids=None, ids_from_chars=None, **config)\n",
    "\n",
    "    def reattach_components(self, model, chars_from_ids, ids_from_chars):\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Recreate the mask\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            values=[-float(\"inf\")] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc64597a-ae7f-47a2-86fb-b368006ad198",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model= model, chars_from_ids=chars_from_ids, ids_from_chars=ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813114d7-f076-4d5a-b0b6-08492a30ed32",
   "metadata": {},
   "source": [
    "Run it in a loop to generate some text. Looking at the generated text, we'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b4a62e-c328-4bbd-9af9-54586309a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Prithee, nuns!\n",
      "\n",
      "JULIET:\n",
      "I pray thy royal and awaking are about,\n",
      "Because those that misposticulish knowledge\n",
      "Oft to our queen and unrest\n",
      "only; nay, look up in prisoner. I till hemp your lordship\n",
      "That my hands have been thy footless armice you,\n",
      "When with myse fine or enough that through allow'd that prevail\n",
      "What concerd id the whiles, looks on thee,\n",
      "But in thy rests that shall be thusbed with me;\n",
      "'Tis no singland cannib.\n",
      "But, Hortensio, go with veint calmmed! Hence with the view of\n",
      "them that respect his subthen can win a doubzed of\n",
      "cheer yourself.\n",
      "Most pleasing yourself in tow. Your houses!\n",
      "Let me see them not such seen to stand.\n",
      "\n",
      "PETRUCHIO:\n",
      "I will, that now mine enemies,\n",
      "sir, about a disconder further than they\n",
      "Within light. We do not sin,\n",
      "That lies a hopel on the shoes amended,\n",
      "Or else safelety of a croldage. He's further\n",
      "As I have let him in his defend.\n",
      "\n",
      "Surse:\n",
      "Right.\n",
      "\n",
      "WARWICK:\n",
      "O Blarance.\n",
      "\n",
      "ANTIGONUS:\n",
      "Who seeman's death, and what lith,\n",
      "I did suppose yourself.\n",
      "\n",
      "AUTOLYCUS:\n",
      "It is the co \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.1389689445495605\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080608c-1d5f-4f97-ab43-1ebc8d7da3f0",
   "metadata": {},
   "source": [
    "If we want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2302d7b2-0c72-4d7f-8991-05deb7bcf0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "But he hath patience!\n",
      "I enjoy no crush: I have spoke, live,\n",
      "Or as they have a second fates,\n",
      "There you to cove on thee; Overfore.\n",
      "That lives. I do speak thee eyes\n",
      "Or else dready temples; flies shall go to you.\n",
      "You are thy dangerous provinces!\n",
      "\n",
      "BRUTUS:\n",
      "Bear the English crown and stay up\n",
      "not then, not forth the sanctuary;\n",
      "Or else have fies' out wine and pays her;\n",
      "Though o'er the names come disposed, would adjumisue\n",
      "That word more wrathful money, may stand\n",
      "The allibour: therefore fares my wife and death.\n",
      "\n",
      "YORK:\n",
      "Ronebrother, welcome done, and were arr from this land:\n",
      "Knock the noise of the thoughts will have more varit more and old\n",
      "too much seat; no warriors wakes,\n",
      "The son--hath poisoned heaven and were none,\n",
      "Have a duley apear or no.\n",
      "\n",
      "DUKE OF YORK:\n",
      "It must your will and dut some confererity\n",
      "often enforced to chyself?\n",
      "\n",
      "POLIXENES:\n",
      "Displate me! that's the gods among?\n",
      "\n",
      "BRUTUS:\n",
      "Said upon the thing is called much.\n",
      "\n",
      "ISABELLA:\n",
      "I pray; the love I never shall have beared my;\n",
      "And while the drums of \n",
      "ROMEO:\n",
      "Alas, that then, give lie those twice than ope thy brother\n",
      "Conerial lords, than you have lies lived.\n",
      "\n",
      "BRUTUS:\n",
      "An' would this, to be it lowed; but I'll touch'd you, I should bite you all your\n",
      "daster in a bondle bust a judge.\n",
      "\n",
      "COMINIUS:\n",
      "By his bisery sfort:\n",
      "As it will stand, night, cause, met her,\n",
      "And so shall acquainted Lords of drone?\n",
      "\n",
      "RICHARD:\n",
      "O, friar, our graces; I have to any of\n",
      "the heavens have said at Carial are me,\n",
      "Become me to thy law and then furness to to$k?\n",
      "\n",
      "ESCALUS:\n",
      "With him that, go thy deast;\n",
      "Or, is it here we heard these officer resolves\n",
      "To Speak blows up your honesty worse.\n",
      "Shrow'd it seems another change; I have any mouth,\n",
      "This is a lord,--hath powers our enemy ears are almost or horse.\n",
      "\n",
      "VINCENTIO:\n",
      "O Grey to save from Cominius and Lancaster\n",
      "Rust talk'd of party.\n",
      "Upon his looks and opposite to thee!\n",
      "\n",
      "PRINCE EDWARD:\n",
      "And so, he hath eaty seal worth thee go.\n",
      "\n",
      "ISABELLA:\n",
      "Upon a perpetch.\n",
      "\n",
      "AUFIDUS:\n",
      "O hope thou hast more!\n",
      "Let them do not; let me defined it;\n",
      "Who, indeed, as fa\n",
      "ROMEO:\n",
      "I know, sir, to use you\n",
      "the greatest senate, at the senain and draggh,\n",
      "Which weals me that the vice shall be a power?\n",
      "\n",
      "BENVOLIO:\n",
      "Bear too late?\n",
      "\n",
      "MENENIUS:\n",
      "Sir, stee is it;\n",
      "One bid children, see, his ears, Hen 'tis\n",
      "about me, and pockaring at the dry unknowledge\n",
      "Out of the dainta's new upon thy soul's\n",
      "As big as oath? Why, this's she mind,--\n",
      "The mild beholding of my person, In hand.\n",
      "\n",
      "KING HENRY VI:\n",
      "Farewell: why dost thou grant may so?\n",
      "Therefore take up to know. Back-with you!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I thank you,--go pity her in arms hath gaveness:\n",
      "In wish, mark not to bear as others change\n",
      "To keep a plot with a free on the world.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Take that with wine.\n",
      "Pincet that just Juliet, and so did?\n",
      "\n",
      "LUCIO:\n",
      "I'll tell you,--\n",
      "This day, we'll know my heart; who, love,\n",
      "That I know you to, give me to you?\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Speak well, Lord Angied may ere hence and maid?\n",
      "\n",
      "VENCY ALWARD:\n",
      "Warwick, shall I use.\n",
      "\n",
      "Shepherd:\n",
      "This, to my cels, and so young prepared the hole.\n",
      "\n",
      "GRUMIO:\n",
      "Fave that did sat s\n",
      "ROMEO:\n",
      "Hark, this tears, I protest, now what we were abusly;\n",
      "A passing fair sonfulate, Thonge of the vanthooat be.\n",
      "\n",
      "BRUTUS:\n",
      "Ay, both naised in the hunt and marter,\n",
      "Twhisper this to hide his purple eye,\n",
      "Even thy Romeo and his spotely fear with him.\n",
      "\n",
      "BUCKINGHAM:\n",
      "No, would we in me to the drumper.\n",
      "What, hadst murder, more, sir; why, holy protector?\n",
      "\n",
      "MENENIUS:\n",
      "Yes, that fit is but nature: therefore I'll parte\n",
      "This is the king and true bears me, as I to\n",
      "death in thy nable: but who loves me bud\n",
      "supper-in, in't is not set on, and she train provers to him,--\n",
      "And this I did find this is.\n",
      "\n",
      "YORK:\n",
      "What, my lord to fram? Come; Friar Froth Rullon!\n",
      "And that's so continuea me swear to the drum.\n",
      "A plear of better from heapen when most virainst, her\n",
      "shall bear it; good my lord.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Sweet sovereign.\n",
      "\n",
      "BIONDELLO:\n",
      "Sir, it is not my blood, this ciunnation\n",
      "We are full of natural; farewe king, what?\n",
      "Nay, but now with a prive other there:\n",
      "God give your trackful next of dreadness.\n",
      "\n",
      "ISABELLA:\n",
      "O, good my lord.\n",
      "\n",
      "ROMEO:\n",
      "How now! whiles I make humour.\n",
      "\n",
      "MARCIUS:\n",
      "O, when you give you, and he shall be most\n",
      "Living liven to be with you; and best\n",
      "Thou wert not interrupt him, and call a pite?\n",
      "Where is thy hair! shall I kise\n",
      "The flating government, which often essage of\n",
      "a jest of death, and cannot Edward?\n",
      "\n",
      "JULIET:\n",
      "I'll tell thee: hear us not, for ever I wind talk,\n",
      "Eethred up about my count Citizen:\n",
      "Nay, but delay the world.\n",
      "\n",
      "BIONDELLO:\n",
      "His daughter is'e.\n",
      "\n",
      "YORK:\n",
      "What think'st thou wilt take my leised father, come,\n",
      "Theiv of our command, with those my porture\n",
      "Of the state after thee: though upon thyself:\n",
      "'Tis true; if it come in anvire it, as your ship\n",
      "is with our perery, years, and excuse.\n",
      "I'll have the friend opprowards than that's tleen.\n",
      "\n",
      "BRUTUS:\n",
      "Why young brother is ere I have bity\n",
      "Of the sublentify to you.\n",
      "\n",
      "MENENIUS:\n",
      "To signify, and let ther all defend and be\n",
      "married to have fetch defending steeds,\n",
      "And in his charge to do't to this.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "This lace, thou'lts I'll go to purchase you.\n",
      "\n",
      "KING RICHARD\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.683785676956177\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "for s in result:\n",
    "    print(s.numpy().decode(\"utf-8\"))\n",
    "print(\"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06e2c1-7ab2-4798-9ad5-71e74434b7d8",
   "metadata": {},
   "source": [
    "## Export the generator\n",
    "\n",
    "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing us to use it anywhere a `tf.saved_model` is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7f6ca32-f41e-4425-b300-381fa1922ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the OneStep model\n",
    "one_step_model.save('one_step_model.keras')\n",
    "\n",
    "# Load the OneStep model\n",
    "one_step_reloaded = tf.keras.models.load_model(\n",
    "    'one_step_model.keras',\n",
    "    custom_objects={'OneStep': OneStep}\n",
    ")\n",
    "\n",
    "# Reattach components\n",
    "one_step_reloaded.reattach_components(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0da04f2a-9270-477b-9314-1883fc4cb9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Or brain! which, doop! take my what lave we say poison,\n",
      "Which we parcillow us the state?\n",
      "What? can \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
